{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946546a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extends to and tests out multivariate functionality and formatting of the variables in the model\n",
    "# Combines synthetic data from mekiv_learning and some of our own synthetic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ed032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c009dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import one of our data sets to work with\n",
    "dat = pd.read_csv('../causal-dim-reduction-text/synthetic_data/data/lda.1-1seed.128len.0.2-0.2delta.0.05-0.05tau.0.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.loc[:,dat.columns!='y'], \n",
    "                                                    dat.y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8d8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA: N\n",
    "# Lemmatization and removing stopwords\n",
    "import nltk\n",
    "# Lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# Stopwords\n",
    "stop_wrds = nltk.corpus.stopwords.words(\"english\")\n",
    "# Preprocess the data\n",
    "# Convert all text to lower case\n",
    "X_train.text = [txt.lower() for txt in X_train.text]\n",
    "# Tokenize the words\n",
    "processed_txt = X_train.text.apply(nltk.tokenize.word_tokenize) \n",
    "# Perform lemmatization\n",
    "processed_txt = processed_txt.apply(lambda x: [lemmatizer.lemmatize(word) for word in x\\\n",
    "                                               if word not in stop_wrds])\n",
    "# BOW representation\n",
    "import gensim\n",
    "# Create a dictionary from the processed text\n",
    "dictionary_txt = gensim.corpora.Dictionary(processed_txt)\n",
    "# Optional: filter out words that appear in fewer than 5 documents or more than 50% of documents\n",
    "# dictionary_txt.filter_extremes(no_below = 5, no_above = 0.5)\n",
    "# Create BOW representation\n",
    "bow_corp = [dictionary_txt.doc2bow(txt) for txt in processed_txt]\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corp,\n",
    "                                       id2word=dictionary_txt,\n",
    "                                       num_topics=10)\n",
    "# Obtain document over topics for new documents\n",
    "# First preprocess the test documents\n",
    "X_test.text = [txt.lower() for txt in X_test.text]\n",
    "processed_test = X_test.text.apply(nltk.tokenize.word_tokenize)\n",
    "processed_test = processed_test.apply(lambda x: [lemmatizer.lemmatize(word) for word in x\\\n",
    "                                                if word not in stop_wrds])\n",
    "test_dict = gensim.corpora.Dictionary(processed_test)\n",
    "bow_corp_test = [test_dict.doc2bow(txt) for txt in processed_test]\n",
    "# Then obtain theta (list of tuples for each doc must be restructured)\n",
    "test_docs_theta = [pd.DataFrame(lda_model.get_document_topics(doc_bow, minimum_probability=1e-7),\\\n",
    "                                columns = ['Topic', 'Proportion'])\\\n",
    "                   for doc_bow in bow_corp_test]\n",
    "test_docs_theta = pd.concat(test_docs_theta)\n",
    "test_docs_theta['Doc_idx'] = [idx for idx in processed_test.index for m in range(10)]\n",
    "test_docs_theta = test_docs_theta.pivot(index = 'Doc_idx', columns='Topic', values='Proportion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16607f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = test_docs_theta.iloc[:,:-1] # Drop last column (not needed since rows sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d527c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlated topic models representation: M\n",
    "import tomotopy as tp\n",
    "\n",
    "# Stem using the porter stemmer \n",
    "porter_stem = nltk.PorterStemmer().stem\n",
    "corp_txt = tp.utils.Corpus(tokenizer = tp.utils.SimpleTokenizer(porter_stem),\n",
    "                          stopwords = stop_wrds)\n",
    "\n",
    "corp_txt.process(txt.lower() for txt in X_train['text'].values.tolist())\n",
    "\n",
    "# IDF: Inverse Document Frequency term weighting (term occurring in almost every document has very low weighting \n",
    "# and a term occurring at a few document has high weighting)\n",
    "\n",
    "ctm_model = tp.CTModel(tw=tp.TermWeight.IDF, k=10, corpus=corp_txt)\n",
    "# tp.CTModel(tw=tp.TermWeight.IDF, min_df=5, rm_top=40, k=10, corpus=corp_txt)\n",
    "\n",
    "ctm_model.train(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "848380cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then obtain theta (list of tuples for each doc must be restructured)\n",
    "prep_ctm_test_docs = [ctm_model.make_doc([x for x in new_doc]) for new_doc in X_test['text']]\n",
    "ctm_test_docs_theta = [ctm_model.infer(test_doc) for test_doc in prep_ctm_test_docs]\n",
    "\n",
    "\n",
    "ctm_test_docs_theta = [pd.DataFrame({'Topic':pd.Series(range(1,11)),'Proportion': test_doc[0]}) \\\n",
    "                       for test_doc in ctm_test_docs_theta]\n",
    "ctm_test_docs_theta = pd.concat(ctm_test_docs_theta)\n",
    "ctm_test_docs_theta['Doc_idx'] = [idx for idx in processed_test.index for m in range(10)]\n",
    "ctm_test_docs_theta = ctm_test_docs_theta.pivot(index = 'Doc_idx', columns='Topic', values='Proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8f04e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = ctm_test_docs_theta.iloc[:,:-1] # Drop last column (not needed since rows sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffe1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4b077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec69ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word2vec representation: M\n",
    "# # Preprocessing\n",
    "# # We can use gensim's built in preprocessing function simple_preprocess to lowercase and tokenize\n",
    "# train_prep = [gensim.utils.simple_preprocess(txt) for txt in X_train.text]\n",
    "# test_prep = [gensim.utils.simple_preprocess(txt) for txt in X_test.text]\n",
    "\n",
    "# # Add tags for training data\n",
    "# tagged_train = [gensim.models.doc2vec.TaggedDocument(doc, [i]) for i,doc in enumerate(train_prep)]\n",
    "# # Create the model\n",
    "# doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size = 50, min_count = 2, epochs = 10)\n",
    "# # Build vocabulary of model\n",
    "# doc2vec_model.build_vocab(tagged_train)\n",
    "# # Train the model\n",
    "# doc2vec_model.train(tagged_train, total_examples = doc2vec_model.corpus_count,\\\n",
    "#                    epochs = doc2vec_model.epochs)\n",
    "# # Predict/infer on test data using the model\n",
    "# doc2vec_test = [doc2vec_model.infer_vector(txt) for txt in test_prep]\n",
    "# # Convert to df\n",
    "# doc2vec_test_df = pd.DataFrame(doc2vec_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890addf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = doc2vec_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82094997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fc89615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data to be used as our variables for everything other than M and N\n",
    "# Following sigmoid_design.py in the data folder\n",
    "\n",
    "def f(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log(np.abs(16 * x - 8) + 1) * np.sign(x - 0.5)\n",
    "\n",
    "\n",
    "# Train data set\n",
    "mu = np.zeros((3,))\n",
    "# mu = torch.zeros((3,))\n",
    "sigma = np.array([[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1]])\n",
    "# sigma = torch.tensor([[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1]])\n",
    "\n",
    "# torch.distributions.MultivariateNormal(torch.zeros((3,)),\\\n",
    "#                                        torch.tensor([[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1]])).sample()\n",
    "\n",
    "\n",
    "from numpy.random import default_rng\n",
    "# rng = default_rng(seed=rand_seed)\n",
    "rng = default_rng(seed=1234)\n",
    "data_size = M.shape[0]\n",
    "utw = rng.multivariate_normal(mu, sigma, size=data_size*N.shape[1])\n",
    "u = utw[:, 0:1]\n",
    "from scipy import stats\n",
    "z = stats.norm.cdf(utw[0:data_size, 2])[:, np.newaxis] \n",
    "Z=z\n",
    "x = stats.norm.cdf(utw[:, 1] + utw[:, 2] / np.sqrt(2))[:, np.newaxis]\n",
    "# x = z + rng.normal(0, 0.01, data_size)[:, np.newaxis]\n",
    "X_hidden=x.reshape(-1, N.shape[1])\n",
    "structural = f(x)\n",
    "Y_struct=structural\n",
    "outcome = f(x) + u\n",
    "Y=outcome\n",
    "\n",
    "# Let's use gaussian error\n",
    "data_size = X_hidden.shape[0]\n",
    "std_X = np.std(X_hidden)\n",
    "# Select scale_m and scale_n\n",
    "# scale_m = 0.25\n",
    "# scale_n = 1\n",
    "# std_M, std_N = std_X * scale_m, std_X * scale_n\n",
    "# M = X_hidden + std_M * np.random.normal(0, 1, data_size)[:, np.newaxis]\n",
    "# N = X_hidden + std_N * np.random.normal(0, 1, data_size)[:, np.newaxis]\n",
    "\n",
    "covariate = None\n",
    "X_obs = None\n",
    "\n",
    "\n",
    "# Test data set\n",
    "x_test = np.linspace(0, 1, 1000*N.shape[1])\n",
    "y_test = f(x_test)\n",
    "X_all_test = x_test.reshape(-1, N.shape[1])\n",
    "Y_struct_test = y_test.reshape(-1, N.shape[1]).sum(axis=1)\n",
    "covariate_test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6d017192",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(0, 1, 1000*N.shape[1])\n",
    "X_all_test = x_test.reshape(-1, N.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9bbd8ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y_struct_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc089e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mekiv method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2662f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "train_params = {'split_ratio': 0.5, \n",
    "                'lambda_mn': [0, -10], \n",
    "                'lambda_n': [0, -10],\n",
    "                'xi': [0, -10],\n",
    "                'lambda_x': None,\n",
    "                'n_chi': 500,\n",
    "                'Chi_lim': [-0.5, 0.5],\n",
    "                'label_cutoff': 1.0,\n",
    "                'reg_param': 0.,\n",
    "                'batch_size': 64, \n",
    "                'lr': 0.1, \n",
    "                'num_epochs': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51eec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***BEGIN STAGE 1***\n",
    "# Obtain training and testing indices\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainset1_idx, trainset2_idx = train_test_split(np.arange(X_hidden.shape[0]),\n",
    "                                                test_size = train_params['split_ratio'],\n",
    "                                                random_state = 1234)\n",
    "Z_trainset1 = Z[trainset1_idx] ; Z_trainset2 = Z[trainset2_idx]\n",
    "M_trainset1 = M.iloc[trainset1_idx,:] ; M_trainset2 = M.iloc[trainset2_idx,:]\n",
    "N_trainset1 = N.iloc[trainset1_idx,:] ; N_trainset2 = N.iloc[trainset2_idx,:]\n",
    "X_hidden_trainset1 = X_hidden[trainset1_idx] ; X_hidden_trainset2 = X_hidden[trainset2_idx]\n",
    "Y_trainset1 = Y[trainset1_idx] ; Y_trainset2 = Y[trainset2_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46966d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c635e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: obtain lambda and gamma via stage1_tuning function (trainer.py file)\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# The stage1_tuning function is used to obtain gamma and lambda\n",
    "# gamma_mn, lambda_mn = self.stage1_tuning(KMN1MN1, KMN1MN2, KZ1Z1, KZ1Z2, lambda_mn)\n",
    "# Method is as follows\n",
    "# Preliminaries\n",
    "# Initialize lambda mn and lambda n\n",
    "lambda_n = np.exp(np.linspace(train_params['lambda_n'][0], train_params['lambda_n'][1], 50))\n",
    "lambda_mn = np.exp(np.linspace(train_params['lambda_mn'][0], train_params['lambda_mn'][1], 50))\n",
    "\n",
    "# Obtain MN (concatenate along second axis)\n",
    "MN_trainset1 = np.c_[M_trainset1, N_trainset1] ; MN_trainset2 = np.c_[M_trainset2, N_trainset2] \n",
    "\n",
    "sigmaN = np.median(cdist(N_trainset1, N_trainset1, \"sqeuclidean\"))\n",
    "sigmaMN = np.median(cdist(MN_trainset1, MN_trainset1, \"sqeuclidean\"))\n",
    "sigmaZ = np.median(cdist(Z_trainset1, Z_trainset1, \"sqeuclidean\"))\n",
    "\n",
    "KZ1Z1 = np.exp(-cdist(Z_trainset1, Z_trainset1, \"sqeuclidean\") / 2 / float(sigmaZ))\n",
    "# torch: KZ1Z1 = torch.exp(-torch.cdist(Z1, Z1, \"sqeuclidean\") / 2 / float(sigmaZ))\n",
    "KZ1Z2 = np.exp(-cdist(Z_trainset1, Z_trainset2, \"sqeuclidean\") / 2 / float(sigmaZ))\n",
    "KN1N1 = np.exp(-cdist(N_trainset1, N_trainset1, \"sqeuclidean\") / 2 / float(sigmaN))\n",
    "KN1N2 = np.exp(-cdist(N_trainset1, N_trainset2, \"sqeuclidean\") / 2 / float(sigmaN))\n",
    "KMN1MN1 = np.exp(-cdist(MN_trainset1, MN_trainset1, \"sqeuclidean\") / 2 / float(sigmaMN))\n",
    "KMN1MN2 = np.exp(-cdist(MN_trainset1, MN_trainset2, \"sqeuclidean\") / 2 / float(sigmaMN))\n",
    "\n",
    "# Calculation\n",
    "n = Z_trainset1.shape[0]\n",
    "# N\n",
    "gamma_list = [np.linalg.solve(KZ1Z1 + n * lam1 * np.eye(n), KZ1Z2) for lam1 in lambda_n]\n",
    "score = [np.trace(gamma.T.dot(KN1N1.dot(gamma)) - 2 * KN1N2.T.dot(gamma)) for gamma in gamma_list]\n",
    "lambda_n = lambda_n[np.argmin(score)]\n",
    "gamma_n = gamma_list[np.argmin(score)]\n",
    "# MN\n",
    "gamma_list = [np.linalg.solve(KZ1Z1 + n * lam1 * np.eye(n), KZ1Z2) for lam1 in lambda_mn]\n",
    "score = [np.trace(gamma.T.dot(KMN1MN1.dot(gamma)) - 2 * KMN1MN2.T.dot(gamma)) for gamma in gamma_list]\n",
    "lambda_mn = lambda_mn[np.argmin(score)]\n",
    "gamma_mn = gamma_list[np.argmin(score)]\n",
    "\n",
    "# ***END STAGE 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e385d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90738146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9265e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***BEGIN Merror STAGE***\n",
    "# This is where the training happens with all the epochs and everything\n",
    "# **1**\n",
    "# The code uses the below function which we instead replace with its entire functionality\n",
    "# stageM_data = create_stage_M_raw_data(self.n_chi, N1, M1, Z2, gamma_n, gamma_mn, sigmaN, KZ1Z2)\n",
    "\n",
    "Chi_n = np.random.normal(0, 1, train_params['n_chi']* (N_trainset1.shape[1]))\n",
    "Chi_n = Chi_n / 2 / np.pi / sigmaN ** 0.5 # because the computed sigmaN is actually sigma^2N\n",
    "Chi_n = Chi_n.reshape(train_params['n_chi'],N_trainset1.shape[1])\n",
    "n, m = KZ1Z2.shape\n",
    "# Columns of Chi are repeated to account for the data size of the variable\n",
    "cos_term = np.cos(Chi_n @ N_trainset1.T)  # shape: Chi.shape[0] x args.train.N.shape[0]\n",
    "sin_term = np.sin(Chi_n @ N_trainset1.T)\n",
    "# Real (cos) and imaginary (sin) parts; dot products with gamma N\n",
    "denom = cos_term.dot(gamma_n) + sin_term.dot(gamma_n) * 1j \n",
    "# Component shape: Chi.shape[0] x args.dev.Z.shape[0]\n",
    "m_gamma_numer = sum([gamma_mn * M_trainset1[i].to_numpy().reshape(-1,1) for i in range(M_trainset1.shape[1])])\n",
    "numer = cos_term.dot(m_gamma_numer) + sin_term.dot(m_gamma_numer) * 1j \n",
    "raw_labels = (numer.to_numpy()/denom.to_numpy()).flatten().reshape(-1,1)\n",
    "raw_Chi = np.repeat(Chi_n, m).reshape(-1, N_trainset1.shape[1])\n",
    "raw_Z = np.repeat(Z_trainset2[np.newaxis, :, :], train_params['n_chi'], axis=0).reshape(-1, Z_trainset2.shape[1])\n",
    "raw_dict = {'labels':raw_labels, 'Chi':raw_Chi, 'Z':raw_Z}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10779a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fcc2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = np.real(raw_dict['labels']).flatten()\n",
    "imag_label = np.imag(raw_dict['labels']).flatten()\n",
    "idx_select = (real_label < np.mean(real_label) + 1. * np.std(real_label)) * (\n",
    "            real_label > np.mean(real_label) - 1. * np.std(real_label)) \\\n",
    "                 * (imag_label < np.mean(imag_label) + 1. * np.std(imag_label)) * (\n",
    "                         imag_label > np.mean(imag_label) - 1. * np.std(imag_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "779911fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78306632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **2**\n",
    "# The code uses the below function which we instead replace with its entire functionality\n",
    "# stageM_data = prepare_stage_M_data(raw_data2=stageM_data, rand_seed=rand_seed)\n",
    "\n",
    "real_label = np.real(raw_dict['labels']).flatten()\n",
    "imag_label = np.imag(raw_dict['labels']).flatten()\n",
    "idx_select = (real_label < np.mean(real_label) + 1. * np.std(real_label)) * (\n",
    "            real_label > np.mean(real_label) - 1. * np.std(real_label)) \\\n",
    "                 * (imag_label < np.mean(imag_label) + 1. * np.std(imag_label)) * (\n",
    "                         imag_label > np.mean(imag_label) - 1. * np.std(imag_label))\n",
    "raw_labels = raw_dict['labels'][idx_select]\n",
    "raw_Chi = raw_dict['Chi'][idx_select]\n",
    "raw_Z = raw_dict['Z'][idx_select]\n",
    "shuffle_idx = np.arange(raw_Z.shape[0])\n",
    "default_rng(seed=1234).shuffle(shuffle_idx)\n",
    "for key in raw_dict.keys():\n",
    "    raw_dict[key][shuffle_idx]\n",
    "# The below code line just converts the data to torch if needed then adds new values to the class\n",
    "# Values added to class and converted to tensors\n",
    "# Pretty sure this isn't needed though because all components are manually converted to tensors in the code \n",
    "# StageMDataSetTorch.from_numpy(raw_data2)\n",
    "stageM_data = {'labels':raw_labels, 'Chi':raw_Chi, 'Z':raw_Z}\n",
    "stage1_MNZ = {'M': M_trainset1.to_numpy(), 'N': N_trainset1.to_numpy(), 'Z': Z_trainset1, 'sigmaZ': sigmaZ}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44535960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f07493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77be540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e99bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c29a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9017769",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_initialiser = (torch.tensor(stage1_MNZ['M'][:,0:stage1_MNZ['N'].shape[1]]) + torch.tensor(stage1_MNZ['N'])) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9e6ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_KZ1Z1 = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], stage1_MNZ['Z'], \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "test_K_Z1z = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], stageM_data['Z'][1:300], \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "test_gamma_x_I_lambda = sum([torch.eye(stage1_MNZ['Z'].shape[0]) * torch.exp(test_x_initialiser[:,i].reshape(-1,1)) for i in range(test_x_initialiser.shape[1])])\n",
    "test_gamma_x = torch.linalg.solve(test_KZ1Z1 + stage1_MNZ['Z'].shape[0] * test_gamma_x_I_lambda, test_K_Z1z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c355c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cos_term = torch.cos(torch.matmul(torch.tensor(stageM_data['Chi'][0:300].reshape(-1,1)).float(),test_x_initialiser.reshape(1, -1)))\n",
    "# test_sin_term = torch.sin(torch.matmul(torch.tensor(stageM_data['Chi'][0:300].reshape(-1,1)).float(),test_x_initialiser.reshape(1, -1)))\n",
    "\n",
    "test_cos_term = [torch.cos(torch.matmul(torch.tensor(stageM_data['Chi'][:,i].reshape(-1,1)[1:300]).float(),\\\n",
    "                                          test_x_initialiser[:,i].reshape(1, -1))) for i in range(test_x_initialiser.shape[1])]\n",
    "test_sin_term = [torch.sin(torch.matmul(torch.tensor(stageM_data['Chi'][:,i].reshape(-1,1)[1:300]).float(),\\\n",
    "                                          test_x_initialiser[:,i].reshape(1, -1))) for i in range(test_x_initialiser.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a229d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2950f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cos_w = torch.sum(test_cos_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "# test_sin_w = torch.sum(test_sin_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "\n",
    "test_cos_w = sum([torch.sum(test_cos_term[i] * test_gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(test_x_initialiser.shape[1])])\n",
    "test_sin_w = sum([torch.sum(test_sin_term[i] * test_gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(test_x_initialiser.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f3ffe7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_w_numer = sum([torch.sum(test_cos_term[i] * test_gamma_x.t() * test_x_initialiser[:,i].reshape(1, -1),\\\n",
    "                                               dim=-1).reshape(-1, 1) for i in range(test_x_initialiser.shape[1])])\n",
    "test_sin_w_numer = sum([torch.sum(test_sin_term[i] * test_gamma_x.t() * test_x_initialiser[:,i].reshape(1, -1),\\\n",
    "                                          dim=-1).reshape(-1, 1) for i in range(test_x_initialiser.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f5872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f040f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88761a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6d6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c13ab19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, batch     1] loss: 0.97909, mse: 0.97909, reg: 0.00000\n",
      "[epoch 1, batch     2] loss: 0.79093, mse: 0.79093, reg: 0.00000\n",
      "[epoch 1, batch     3] loss: 0.62362, mse: 0.62362, reg: 0.00000\n",
      "[epoch 1, batch     4] loss: 0.47784, mse: 0.47784, reg: 0.00000\n",
      "[epoch 1, batch     5] loss: 0.35185, mse: 0.35185, reg: 0.00000\n",
      "[epoch 1, batch     6] loss: 0.24783, mse: 0.24783, reg: 0.00000\n",
      "[epoch 1, batch     7] loss: 0.16296, mse: 0.16296, reg: 0.00000\n",
      "[epoch 1, batch     8] loss: 0.09829, mse: 0.09829, reg: 0.00000\n",
      "[epoch 1, batch     9] loss: 0.05099, mse: 0.05099, reg: 0.00000\n",
      "[epoch 1, batch    10] loss: 0.02131, mse: 0.02131, reg: 0.00000\n",
      "[epoch 1, batch    11] loss: 0.00453, mse: 0.00453, reg: 0.00000\n",
      "[epoch 1, batch    12] loss: 0.00014, mse: 0.00014, reg: 0.00000\n",
      "[epoch 1, batch    13] loss: 0.00445, mse: 0.00445, reg: 0.00000\n",
      "[epoch 1, batch    14] loss: 0.01488, mse: 0.01488, reg: 0.00000\n",
      "[epoch 1, batch    15] loss: 0.02878, mse: 0.02878, reg: 0.00000\n",
      "[epoch 1, batch    16] loss: 0.04219, mse: 0.04219, reg: 0.00000\n",
      "[epoch 1, batch    17] loss: 0.05590, mse: 0.05590, reg: 0.00000\n",
      "[epoch 1, batch    18] loss: 0.06564, mse: 0.06564, reg: 0.00000\n",
      "[epoch 1, batch    19] loss: 0.07155, mse: 0.07155, reg: 0.00000\n",
      "[epoch 1, batch    20] loss: 0.07395, mse: 0.07395, reg: 0.00000\n",
      "[epoch 1, batch    21] loss: 0.07178, mse: 0.07178, reg: 0.00000\n",
      "[epoch 1, batch    22] loss: 0.06686, mse: 0.06686, reg: 0.00000\n",
      "[epoch 1, batch    23] loss: 0.05941, mse: 0.05941, reg: 0.00000\n",
      "[epoch 1, batch    24] loss: 0.05041, mse: 0.05041, reg: 0.00000\n",
      "[epoch 1, batch    25] loss: 0.03992, mse: 0.03992, reg: 0.00000\n",
      "[epoch 1, batch    26] loss: 0.03052, mse: 0.03052, reg: 0.00000\n",
      "[epoch 1, batch    27] loss: 0.02130, mse: 0.02130, reg: 0.00000\n",
      "[epoch 1, batch    28] loss: 0.01329, mse: 0.01329, reg: 0.00000\n",
      "[epoch 1, batch    29] loss: 0.00735, mse: 0.00735, reg: 0.00000\n",
      "[epoch 1, batch    30] loss: 0.00307, mse: 0.00307, reg: 0.00000\n",
      "[epoch 1, batch    31] loss: 0.00080, mse: 0.00080, reg: 0.00000\n",
      "[epoch 1, batch    32] loss: 0.00004, mse: 0.00004, reg: 0.00000\n",
      "[epoch 1, batch    33] loss: 0.00059, mse: 0.00059, reg: 0.00000\n",
      "[epoch 1, batch    34] loss: 0.00207, mse: 0.00207, reg: 0.00000\n",
      "[epoch 1, batch    35] loss: 0.00414, mse: 0.00414, reg: 0.00000\n",
      "[epoch 1, batch    36] loss: 0.00609, mse: 0.00609, reg: 0.00000\n",
      "[epoch 1, batch    37] loss: 0.00798, mse: 0.00798, reg: 0.00000\n",
      "[epoch 1, batch    38] loss: 0.00961, mse: 0.00961, reg: 0.00000\n",
      "[epoch 1, batch    39] loss: 0.01041, mse: 0.01041, reg: 0.00000\n",
      "[epoch 1, batch    40] loss: 0.01040, mse: 0.01040, reg: 0.00000\n",
      "[epoch 1, batch    41] loss: 0.00999, mse: 0.00999, reg: 0.00000\n",
      "[epoch 1, batch    42] loss: 0.00899, mse: 0.00899, reg: 0.00000\n",
      "[epoch 1, batch    43] loss: 0.00763, mse: 0.00763, reg: 0.00000\n",
      "[epoch 1, batch    44] loss: 0.00596, mse: 0.00596, reg: 0.00000\n",
      "[epoch 1, batch    45] loss: 0.00435, mse: 0.00435, reg: 0.00000\n",
      "[epoch 1, batch    46] loss: 0.00292, mse: 0.00292, reg: 0.00000\n",
      "[epoch 1, batch    47] loss: 0.00168, mse: 0.00168, reg: 0.00000\n",
      "[epoch 1, batch    48] loss: 0.00078, mse: 0.00078, reg: 0.00000\n",
      "[epoch 1, batch    49] loss: 0.00025, mse: 0.00025, reg: 0.00000\n",
      "[epoch 1, batch    50] loss: 0.00003, mse: 0.00003, reg: 0.00000\n",
      "[epoch 1, batch    51] loss: 0.00009, mse: 0.00009, reg: 0.00000\n",
      "[epoch 1, batch    52] loss: 0.00036, mse: 0.00036, reg: 0.00000\n",
      "[epoch 1, batch    53] loss: 0.00073, mse: 0.00073, reg: 0.00000\n",
      "[epoch 1, batch    54] loss: 0.00112, mse: 0.00112, reg: 0.00000\n",
      "[epoch 1, batch    55] loss: 0.00147, mse: 0.00147, reg: 0.00000\n",
      "[epoch 1, batch    56] loss: 0.00173, mse: 0.00173, reg: 0.00000\n",
      "[epoch 1, batch    57] loss: 0.00186, mse: 0.00186, reg: 0.00000\n",
      "[epoch 1, batch    58] loss: 0.00180, mse: 0.00180, reg: 0.00000\n",
      "[epoch 1, batch    59] loss: 0.00162, mse: 0.00162, reg: 0.00000\n",
      "[epoch 1, batch    60] loss: 0.00139, mse: 0.00139, reg: 0.00000\n",
      "[epoch 1, batch    61] loss: 0.00113, mse: 0.00113, reg: 0.00000\n",
      "[epoch 1, batch    62] loss: 0.00079, mse: 0.00079, reg: 0.00000\n",
      "[epoch 1, batch    63] loss: 0.00058, mse: 0.00058, reg: 0.00000\n",
      "[epoch 1, batch    64] loss: 0.00031, mse: 0.00031, reg: 0.00000\n",
      "[epoch 1, batch    65] loss: 0.00016, mse: 0.00016, reg: 0.00000\n",
      "[epoch 1, batch    66] loss: 0.00006, mse: 0.00006, reg: 0.00000\n",
      "[epoch 1, batch    67] loss: 0.00003, mse: 0.00003, reg: 0.00000\n",
      "[epoch 1, batch    68] loss: 0.00006, mse: 0.00006, reg: 0.00000\n",
      "[epoch 1, batch    69] loss: 0.00013, mse: 0.00013, reg: 0.00000\n",
      "[epoch 1, batch    70] loss: 0.00023, mse: 0.00023, reg: 0.00000\n",
      "[epoch 1, batch    71] loss: 0.00027, mse: 0.00027, reg: 0.00000\n",
      "[epoch 1, batch    72] loss: 0.00031, mse: 0.00031, reg: 0.00000\n",
      "[epoch 1, batch    73] loss: 0.00039, mse: 0.00039, reg: 0.00000\n",
      "[epoch 1, batch    74] loss: 0.00041, mse: 0.00041, reg: 0.00000\n",
      "[epoch 1, batch    75] loss: 0.00034, mse: 0.00034, reg: 0.00000\n",
      "[epoch 1, batch    76] loss: 0.00031, mse: 0.00031, reg: 0.00000\n",
      "[epoch 1, batch    77] loss: 0.00025, mse: 0.00025, reg: 0.00000\n",
      "[epoch 1, batch    78] loss: 0.00017, mse: 0.00017, reg: 0.00000\n",
      "[epoch 1, batch    79] loss: 0.00013, mse: 0.00013, reg: 0.00000\n",
      "[epoch 1, batch    80] loss: 0.00006, mse: 0.00006, reg: 0.00000\n",
      "[epoch 1, batch    81] loss: 0.00004, mse: 0.00004, reg: 0.00000\n",
      "[epoch 1, batch    82] loss: 0.00003, mse: 0.00003, reg: 0.00000\n",
      "[epoch 1, batch    83] loss: 0.00002, mse: 0.00002, reg: 0.00000\n",
      "[epoch 1, batch    84] loss: 0.00005, mse: 0.00005, reg: 0.00000\n",
      "[epoch 1, batch    85] loss: 0.00004, mse: 0.00004, reg: 0.00000\n",
      "[epoch 1, batch    86] loss: 0.00007, mse: 0.00007, reg: 0.00000\n",
      "[epoch 1, batch    87] loss: 0.00008, mse: 0.00008, reg: 0.00000\n",
      "[epoch 1, batch    88] loss: 0.00008, mse: 0.00008, reg: 0.00000\n",
      "[epoch 1, batch    89] loss: 0.00009, mse: 0.00009, reg: 0.00000\n",
      "[epoch 1, batch    90] loss: 0.00009, mse: 0.00009, reg: 0.00000\n",
      "[epoch 1, batch    91] loss: 0.00009, mse: 0.00009, reg: 0.00000\n",
      "[epoch 1, batch    92] loss: 0.00007, mse: 0.00007, reg: 0.00000\n",
      "[epoch 1, batch    93] loss: 0.00008, mse: 0.00008, reg: 0.00000\n",
      "[epoch 1, batch    94] loss: 0.00005, mse: 0.00005, reg: 0.00000\n",
      "[epoch 1, batch    95] loss: 0.00003, mse: 0.00003, reg: 0.00000\n",
      "[epoch 1, batch    96] loss: 0.00002, mse: 0.00002, reg: 0.00000\n",
      "[epoch 1, batch    97] loss: 0.00002, mse: 0.00002, reg: 0.00000\n",
      "[epoch 1, batch    98] loss: 0.00002, mse: 0.00002, reg: 0.00000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# **3** \n",
    "\n",
    "# stage_m_out = self.stage_M_main(stageM_data=stageM_data, stage1_MNZ=stage1_MNZ, train_params=self.train_params)\n",
    "class model_class(torch.nn.Module):\n",
    "    def __init__(self, stageM_data: stageM_data, train_params: train_params, stage1_MNZ: stage1_MNZ,\n",
    "                 gpu_flg: bool = False):\n",
    "        super().__init__()\n",
    "        self.stageM_data = stageM_data\n",
    "        self.stage1_MNZ = stage1_MNZ\n",
    "        self.reg_param = train_params['reg_param']\n",
    "        # We are attempting to uncover a 1 dimensional X and thus initialize with the row averages of M and N \n",
    "#       self.x_initialiser = (torch.tensor(stage1_MNZ['M']).mean(axis=1) + torch.tensor(stage1_MNZ['N']).mean(axis=1)) / 2\n",
    "        # Multidimensional X with the same dimensions as N    \n",
    "        self.x_initialiser = (torch.tensor(stage1_MNZ['M'][:,0:stage1_MNZ['N'].shape[1]]) + torch.tensor(stage1_MNZ['N'])) / 2\n",
    "\n",
    "        if not train_params['lambda_x']:\n",
    "#             self.params = torch.nn.Parameter(self.x_initialiser.flatten())\n",
    "#             self.x = self.params.reshape(-1,1)\n",
    "            self.x = torch.nn.Parameter(self.x_initialiser)\n",
    "            self.lambda_x = self.x\n",
    "        else:\n",
    "#             self.params = torch.nn.Parameter(self.x_initialiser.flatten())\n",
    "#             self.x = self.params.reshape(-1,1)\n",
    "            self.x = torch.nn.Parameter(self.x_initialiser)\n",
    "            self.lambda_x = train_params['lambda_x']\n",
    "        self.train_params = train_params\n",
    "        self.KZ1Z1 = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], stage1_MNZ['Z'], \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "    def forward(self, idx):\n",
    "        ### gamma ###\n",
    "        n = self.stage1_MNZ['Z'].shape[0]\n",
    "        z = self.stageM_data['Z'][idx]\n",
    "        K_Z1z = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], z, \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "        # gamma = self.cme_X.brac_inv.matmul(K_Zz)\n",
    "        if not self.train_params[\"lambda_x\"]:\n",
    "            gamma_x_I_lambda = sum([torch.eye(n) * torch.exp(self.lambda_x[:,i].reshape(-1,1)) for i in range(self.lambda_x.shape[1])])\n",
    "            gamma_x = torch.linalg.solve(self.KZ1Z1 + n * gamma_x_I_lambda, K_Z1z)\n",
    "#             gamma_x = torch.linalg.solve(self.KZ1Z1 + n * torch.exp(self.lambda_x) * torch.eye(n), K_Z1z)\n",
    "            # gamma_x = torch.linalg.solve(self.KZ1Z1 + n * self.lambda_x * torch.eye(n), K_Z1z)\n",
    "        else:\n",
    "            gamma_x_I_lambda = sum([torch.eye(n) * torch.exp(self.lambda_x[:,i].reshape(-1,1)) for i in range(self.lambda_x.shape[1])])\n",
    "            gamma_x = torch.linalg.solve(self.KZ1Z1 + n * gamma_x_I_lambda, K_Z1z)\n",
    "#             gamma_x = torch.linalg.solve(self.KZ1Z1 + n * self.lambda_x * torch.eye(n), K_Z1z)\n",
    "\n",
    "\n",
    "        ### decompose e^{i\\mathcal{X}n_i} ###\n",
    "        cos_term = [torch.cos(torch.matmul(torch.tensor(self.stageM_data['Chi'][:,i].reshape(-1,1)[idx]).float(),\\\n",
    "                                          self.x[:,i].reshape(1, -1))) for i in range(self.x.shape[1])]\n",
    "        sin_term = [torch.sin(torch.matmul(torch.tensor(self.stageM_data['Chi'][:,i].reshape(-1,1)[idx]).float(),\\\n",
    "                                          self.x[:,i].reshape(1, -1))) for i in range(self.x.shape[1])]\n",
    "#         cos_term = torch.cos(torch.matmul(torch.tensor(self.stageM_data['Chi'].reshape(-1,1)[idx]).float(),\\\n",
    "#                                           self.x.reshape(1, -1)))\n",
    "#         sin_term = torch.sin(torch.matmul(torch.tensor(self.stageM_data['Chi'].reshape(-1,1)[idx]).float(),\\\n",
    "#                                           self.x.reshape(1, -1)))\n",
    "\n",
    "        denom = {}\n",
    "        # using gamma to evaluate the charasteristic function value at a bunch of curly_x's\n",
    "        denom['cos_weighted'] = sum([torch.sum(cos_term[i] * gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(self.x.shape[1])])\n",
    "        denom['sin_weighted'] = sum([torch.sum(sin_term[i] * gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(self.x.shape[1])])\n",
    "#         denom['cos_weighted'] = torch.sum(cos_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "#         denom['sin_weighted'] = torch.sum(sin_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "        denom['value'] = denom['cos_weighted'] + denom['sin_weighted'] * 1j\n",
    "\n",
    "        numer = {}\n",
    "        numer['cos_weighted'] = sum([torch.sum(cos_term[i] * gamma_x.t() * self.x[:,i].reshape(1, -1),\\\n",
    "                                               dim=-1).reshape(-1, 1) for i in range(self.x.shape[1])])\n",
    "        numer['sin_weighted'] = sum([torch.sum(sin_term[i] * gamma_x.t() * self.x[:,i].reshape(1, -1),\\\n",
    "                                          dim=-1).reshape(-1, 1) for i in range(self.x.shape[1])])\n",
    "#         numer['cos_weighted'] = torch.sum(cos_term * gamma_x.t() * self.x.reshape(1, -1), dim=-1).reshape(-1, 1)\n",
    "#         numer['sin_weighted'] = torch.sum(sin_term * gamma_x.t() * self.x.reshape(1, -1), dim=-1).reshape(-1, 1)\n",
    "        numer['value'] = numer['cos_weighted'] + numer['sin_weighted'] * 1j\n",
    "\n",
    "        return numer['value'] / denom['value']\n",
    "\n",
    "model = model_class(stageM_data=stageM_data, train_params=train_params, stage1_MNZ=stage1_MNZ)\n",
    "\n",
    "model.train() # tells your model that you are training the model, not evaluating it\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_params['lr'])\n",
    "\n",
    "losses = []\n",
    "early_stop = False\n",
    "step = 0\n",
    "for ep in range(train_params['num_epochs']):\n",
    "    if early_stop:\n",
    "        break\n",
    "    running_loss = 0.0\n",
    "    batches_idxes = []\n",
    "    idxes = np.arange((stageM_data['Chi']).shape[0])\n",
    "    np.random.shuffle(idxes)\n",
    "    batch_i = 0\n",
    "    while True:\n",
    "        batches_idxes.append(torch.tensor(idxes[batch_i * train_params['batch_size']: (batch_i + 1) * train_params['batch_size']]))\n",
    "        batch_i += 1\n",
    "        if batch_i * train_params['batch_size'] >= (stageM_data['Chi']).shape[0]:\n",
    "            break\n",
    "    for i, batch_idx in enumerate(batches_idxes):\n",
    "        preds = model(batch_idx)\n",
    "        # Loss functionality\n",
    "        labels = stageM_data['labels'][batch_idx]\n",
    "        dim_label = labels.shape[-1]\n",
    "        num_label = labels.shape[0]\n",
    "        preds_as_real = torch.view_as_real(preds)\n",
    "        labels_as_real = torch.view_as_real(torch.tensor(labels))\n",
    "        mse = torch.sum((labels_as_real - preds_as_real) ** 2) / num_label / dim_label\n",
    "#       reg = torch.sum((model.x - (torch.tensor(stage1_MNZ['M'].mean(axis=1) + stage1_MNZ['N'].mean(axis=1)) / 2)) ** 2)\n",
    "        reg = torch.sum((model.x - (torch.tensor(stage1_MNZ['M'][:,0:stage1_MNZ['N'].shape[1]] + stage1_MNZ['N']) / 2))** 2)\n",
    "        loss = mse + train_params['reg_param'] * reg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print('[epoch %d, batch %5d] loss: %.5f, mse: %.5f, reg: %.5f' % (\n",
    "            ep + 1, i + 1, running_loss / 1, mse / 1, train_params['reg_param'] * reg / 1))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if step > 8000: # Max of 8000 iterations\n",
    "            break\n",
    "        if (step > 2) and np.abs(losses[-1] - losses[-2]) < 1e-7: # Convergence considered to be < 1e-7\n",
    "            early_stop = True\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "\n",
    "# Convert model to numpy after training\n",
    "fitted_x = model.x.detach().numpy()\n",
    "#  assert stage_M_out.fitted_x.shape[0] == stage1_MNZ.Z.shape[0]\n",
    "if not train_params['lambda_x']:\n",
    "    lambda_x = np.exp(model.lambda_x.detach().numpy())  # syntax?\n",
    "else:\n",
    "    lambda_x = model.lambda_x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1a2d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **4**\n",
    "gamma_x = sum([np.linalg.solve(KZ1Z1 + n * lambda_x[:,i] * np.eye(n), KZ1Z2) for i in range(lambda_x.shape[1])])\n",
    "#gamma_x = np.linalg.solve(KZ1Z1 + n * lambda_x * np.eye(n), KZ1Z2)\n",
    "sigmaX = np.median(cdist(fitted_x, fitted_x, \"sqeuclidean\"))\n",
    "KfittedX = np.exp(-cdist(fitted_x, fitted_x, \"sqeuclidean\") / 2 / float(sigmaX))\n",
    "W = KfittedX.dot(gamma_x)\n",
    "\n",
    "# ***END Merror STAGE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71541bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9f34bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***BEGIN STAGE 2***\n",
    "\n",
    "xi = train_params['xi']\n",
    "if isinstance(xi, list):\n",
    "    xi = np.exp(np.linspace(xi[0], xi[1], 50))\n",
    "    M = W.shape[1]\n",
    "    b = W.dot(Y_trainset2)\n",
    "    A = W.dot(W.T) # W.T is transpose of W\n",
    "    alpha_list = [np.linalg.solve(A + M * lam2 * KfittedX, b) for lam2 in xi]\n",
    "    score = [np.linalg.norm(Y_trainset1 - KfittedX.dot(alpha)) for alpha in alpha_list]\n",
    "    alpha = alpha_list[np.argmin(score)]\n",
    "    xi = xi[np.argmin(score)]\n",
    "else:\n",
    "    alpha = np.linalg.solve(W.dot(W.T) + m * self.xi * KfittedX, W.dot(Y_trainset2))\n",
    "    \n",
    "# ***END STAGE 2***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "21a22ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***OBTAIN FINAL OUTPUT***\n",
    "# Concatenate the covariate with the test data if there is a covariate\n",
    "if covariate_test is not None:\n",
    "    X_all_test = np.concatenate([X_all_test, covariate_test], axis=-1)\n",
    "# Obtain predictions \n",
    "Kx = np.exp(-cdist(X_all_test, fitted_x, \"sqeuclidean\") / 2 / float(sigmaX))\n",
    "preds = np.dot(Kx, alpha)\n",
    "# Evaluate the model \n",
    "mse = np.mean((Y_struct_test - preds)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba63e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c71babd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201.5096159526795\n"
     ]
    }
   ],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21828539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4baaf095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4876152472680255\n"
     ]
    }
   ],
   "source": [
    "# Old mse (1 dimensional X)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982d2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
