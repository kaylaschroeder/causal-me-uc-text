{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946546a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extends to and tests out multivariate functionality and formatting of the variables in the model\n",
    "# Combines synthetic data from mekiv_learning and some of our own synthetic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ed032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c009dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import synthetic data set to work with\n",
    "dat = pd.read_csv('synthetic_data/lda.1-1seed.128len.0.1-0.2delta.0.6-0.5tau.0.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test for fitting LDA and CTM. We use 5000 observations for this\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.loc[:,dat.columns!='y'], \n",
    "                                                    dat.y, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5b822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1ae09d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in saved N\n",
    "N = pd.read_csv('synthetic_data/N.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8d8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If N has not been obtained already, use the below to obtain N\n",
    "# LDA: N\n",
    "# Lemmatization and removing stopwords\n",
    "import nltk\n",
    "# Lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# Stopwords\n",
    "stop_wrds = nltk.corpus.stopwords.words(\"english\")\n",
    "# Preprocess the data\n",
    "# Convert all text to lower case\n",
    "X_train.text = [txt.lower() for txt in X_train.text]\n",
    "# Tokenize the words\n",
    "processed_txt = X_train.text.apply(nltk.tokenize.word_tokenize) \n",
    "# Perform lemmatization\n",
    "processed_txt = processed_txt.apply(lambda x: [lemmatizer.lemmatize(word) for word in x\\\n",
    "                                               if word not in stop_wrds])\n",
    "# BOW representation\n",
    "import gensim\n",
    "# Create a dictionary from the processed text\n",
    "dictionary_txt = gensim.corpora.Dictionary(processed_txt)\n",
    "# Optional: filter out words that appear in fewer than 5 documents or more than 50% of documents\n",
    "# dictionary_txt.filter_extremes(no_below = 5, no_above = 0.5)\n",
    "# Create BOW representation\n",
    "bow_corp = [dictionary_txt.doc2bow(txt) for txt in processed_txt]\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corp,\n",
    "                                   id2word=dictionary_txt,\n",
    "                                   num_topics=200,\n",
    "                                   random_state = 1234)\n",
    "# Obtain document over topics for new documents\n",
    "# First preprocess the test documents\n",
    "X_test.text = [txt.lower() for txt in X_test.text]\n",
    "processed_test = X_test.text.apply(nltk.tokenize.word_tokenize)\n",
    "processed_test = processed_test.apply(lambda x: [lemmatizer.lemmatize(word) for word in x\\\n",
    "                                                if word not in stop_wrds])\n",
    "test_dict = gensim.corpora.Dictionary(processed_test)\n",
    "bow_corp_test = [test_dict.doc2bow(txt) for txt in processed_test]\n",
    "# Then obtain theta (list of tuples for each doc must be restructured)\n",
    "test_docs_theta = [pd.DataFrame(lda_model.get_document_topics(doc_bow, minimum_probability=1e-7),\\\n",
    "                                columns = ['Topic', 'Proportion'])\\\n",
    "                   for doc_bow in bow_corp_test]\n",
    "test_docs_theta = pd.concat(test_docs_theta)\n",
    "test_docs_theta['Doc_idx'] = [idx for idx in processed_test.index for m in range(200)]\n",
    "test_docs_theta = test_docs_theta.pivot(index = 'Doc_idx', columns='Topic', values='Proportion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16607f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = test_docs_theta.iloc[:,:-1] # Drop last column (not needed since rows sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9832f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for future use\n",
    "N.to_csv('synthetic_data/N.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940e7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c0853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4447f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in saved M\n",
    "M = pd.read_csv('synthetic_data/M.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a21e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For M, train an LDA model with a different seed\n",
    "lda_model2 = gensim.models.LdaModel(corpus=bow_corp,\n",
    "                                   id2word=dictionary_txt,\n",
    "                                   num_topics=200,\n",
    "                                   random_state = 123)\n",
    "# Obtain document over topics for new documents\n",
    "# Then obtain theta (list of tuples for each doc must be restructured)\n",
    "test_docs_theta2 = [pd.DataFrame(lda_model2.get_document_topics(doc_bow, minimum_probability=1e-7),\\\n",
    "                                columns = ['Topic', 'Proportion'])\\\n",
    "                   for doc_bow in bow_corp_test]\n",
    "test_docs_theta2 = pd.concat(test_docs_theta2)\n",
    "test_docs_theta2['Doc_idx'] = [idx for idx in processed_test.index for m in range(200)]\n",
    "test_docs_theta2 = test_docs_theta2.pivot(index = 'Doc_idx', columns='Topic', values='Proportion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d527c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If M has not been obtained already, use the below to obtain M\n",
    "# # Correlated topic models representation: M\n",
    "# import tomotopy as tp\n",
    "# import nltk\n",
    "\n",
    "# # Stem using the porter stemmer \n",
    "# stop_wrds = nltk.corpus.stopwords.words(\"english\")\n",
    "# porter_stem = nltk.PorterStemmer().stem\n",
    "# corp_txt = tp.utils.Corpus(tokenizer = tp.utils.SimpleTokenizer(porter_stem),\n",
    "#                           stopwords = stop_wrds)\n",
    "\n",
    "# corp_txt.process(txt.lower() for txt in X_train['text'].values.tolist())\n",
    "\n",
    "# # IDF: Inverse Document Frequency term weighting (term occurring in almost every document has very low weighting \n",
    "# # and a term occurring at a few document has high weighting)\n",
    "\n",
    "# ctm_model = tp.CTModel(tw=tp.TermWeight.IDF, k=200, corpus=corp_txt)\n",
    "# # tp.CTModel(tw=tp.TermWeight.IDF, min_df=5, rm_top=40, k=10, corpus=corp_txt)\n",
    "\n",
    "# # for i in range(0, 100, 10):\n",
    "# #     ctm_model.train(10)\n",
    "# #     print('Iteration: {}\\tLog-likelihood: {}'.format(i, ctm_model.ll_per_word))\n",
    "# ctm_model.train(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848380cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the test documents\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# X_test.text = [txt.lower() for txt in X_test.text]\n",
    "# processed_test = X_test.text.apply(nltk.tokenize.word_tokenize)\n",
    "# processed_test = processed_test.apply(lambda x: [lemmatizer.lemmatize(word) for word in x\\\n",
    "#                                                 if word not in stop_wrds])\n",
    "\n",
    "# # Then obtain theta (list of tuples for each doc must be restructured)\n",
    "# prep_ctm_test_docs = [ctm_model.make_doc([x for x in new_doc]) for new_doc in X_test['text']]\n",
    "# ctm_test_docs_theta = [ctm_model.infer(test_doc) for test_doc in prep_ctm_test_docs]\n",
    "\n",
    "\n",
    "# ctm_test_docs_theta = [pd.DataFrame({'Topic':pd.Series(range(1,201)),'Proportion': test_doc[0]}) \\\n",
    "#                        for test_doc in ctm_test_docs_theta]\n",
    "# ctm_test_docs_theta = pd.concat(ctm_test_docs_theta)\n",
    "# ctm_test_docs_theta['Doc_idx'] = [idx for idx in processed_test.index for m in range(200)]\n",
    "# ctm_test_docs_theta = ctm_test_docs_theta.pivot(index = 'Doc_idx', columns='Topic', values='Proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f04e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = ctm_test_docs_theta.iloc[:,:-1] # Drop last column (not needed since rows sum to 1)\n",
    "M = test_docs_theta2.iloc[:,:-1] # Drop last column (not needed since rows sum to 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ffe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for future use\n",
    "M.to_csv('synthetic_data/M.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c02ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ae4b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder topics of M to match topic ordering in N\n",
    "# We use cosine similarity to match topics \n",
    "# order_topics_m = [-1]*M.shape[1]\n",
    "# for m_idx in range(M.shape[1]):\n",
    "#     sim_max = np.dot(M.iloc[:,m_idx],N.iloc[:,0])/(np.linalg.norm(M.iloc[:,m_idx])*np.linalg.norm(N.iloc[:,0]))\n",
    "#         sim_max = np.dot(M.iloc[:,m_idx],N.iloc[:,0])/(np.linalg.norm(M.iloc[:,m_idx])*np.linalg.norm(N.iloc[:,0]))\n",
    "#     corresponding_idx = 0\n",
    "#     for n_idx in range(N.shape[1]-1):\n",
    "# #         new_sim = np.dot(M.iloc[:,m_idx],N.iloc[:,n_idx+1])/(np.linalg.norm(M.iloc[:,m_idx])*np.linalg.norm(N.iloc[:,n_idx+1]))\n",
    "#         new_sim = np.dot(M.iloc[:,m_idx],N.iloc[:,n_idx+1])/(np.linalg.norm(M.iloc[:,m_idx])*np.linalg.norm(N.iloc[:,n_idx+1]))\n",
    "#         if new_sim > sim_max:\n",
    "#             corresponding_idx = n_idx\n",
    "#             sim_max = new_sim\n",
    "#     order_topics_m[m_idx] = corresponding_idx\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94871861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox for topic matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9cc2bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37187004124046685\n",
      "0.3860158931428285\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(M.iloc[:,89],N.iloc[:,0])/(np.linalg.norm(M.iloc[:,89])*np.linalg.norm(N.iloc[:,0])))\n",
    "print(np.dot(M.iloc[:,120],N.iloc[:,0])/(np.linalg.norm(M.iloc[:,120])*np.linalg.norm(N.iloc[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6a9dd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ([np.dot(M.iloc[:,1],N.iloc[:,n_idx])/(np.linalg.norm(M.iloc[:,1])*np.linalg.norm(N.iloc[:,n_idx]))\\\n",
    "#  for n_idx in range(N.shape[1])])[80:95]\n",
    "test_dist = [np.dot(M.iloc[:,89],N.iloc[:,n_idx])/(np.linalg.norm(M.iloc[:,89])*np.linalg.norm(N.iloc[:,n_idx]))\\\n",
    "              for n_idx in range(N.shape[1])]\n",
    "max(test_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154399ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ad897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec69ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word2vec representation: M\n",
    "# # Preprocessing\n",
    "# # We can use gensim's built in preprocessing function simple_preprocess to lowercase and tokenize\n",
    "# train_prep = [gensim.utils.simple_preprocess(txt) for txt in X_train.text]\n",
    "# test_prep = [gensim.utils.simple_preprocess(txt) for txt in X_test.text]\n",
    "\n",
    "# # Add tags for training data\n",
    "# tagged_train = [gensim.models.doc2vec.TaggedDocument(doc, [i]) for i,doc in enumerate(train_prep)]\n",
    "# # Create the model\n",
    "# doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size = 50, min_count = 2, epochs = 10)\n",
    "# # Build vocabulary of model\n",
    "# doc2vec_model.build_vocab(tagged_train)\n",
    "# # Train the model\n",
    "# doc2vec_model.train(tagged_train, total_examples = doc2vec_model.corpus_count,\\\n",
    "#                    epochs = doc2vec_model.epochs)\n",
    "# # Predict/infer on test data using the model\n",
    "# doc2vec_test = [doc2vec_model.infer_vector(txt) for txt in test_prep]\n",
    "# # Convert to df\n",
    "# doc2vec_test_df = pd.DataFrame(doc2vec_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890addf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = doc2vec_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82094997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db961e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53246373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old data split setup\n",
    "# # Obtain Z for training, and hidden X and Y for both training and testing\n",
    "# Z_X_hidden, Z_X_hidden_test, Y, Y_test = train_test_split(X_test, y_test, \\\n",
    "#                                                           test_size=0.5, random_state=1234)\n",
    "\n",
    "# Z = pd.DataFrame(Z_X_hidden.loc[:,'z'])\n",
    "# X_hidden1 = Z_X_hidden.iloc[:,3:]\n",
    "# X_hidden_test = Z_X_hidden_test.iloc[:,3:]\n",
    "# # No covariates used \n",
    "# covariate_test = None\n",
    "\n",
    "# # Convert Y and Y_test to data frames\n",
    "# Y = pd.DataFrame(Y)\n",
    "# Y_test = pd.DataFrame(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e92529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fc89615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data to be used as our variables for everything other than M and N\n",
    "# Following sigmoid_design.py in the data folder\n",
    "\n",
    "# def f(x: np.ndarray) -> np.ndarray:\n",
    "#     return np.log(np.abs(16 * x - 8) + 1) * np.sign(x - 0.5)\n",
    "\n",
    "\n",
    "# # Train data set\n",
    "# mu = np.zeros((3,))\n",
    "# # mu = torch.zeros((3,))\n",
    "# sigma = np.array([[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1]])\n",
    "# # sigma = torch.tensor([[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1]])\n",
    "\n",
    "# # torch.distributions.MultivariateNormal(torch.zeros((3,)),\\\n",
    "# #                                        torch.tensor([[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1]])).sample()\n",
    "\n",
    "\n",
    "# from numpy.random import default_rng\n",
    "# # rng = default_rng(seed=rand_seed)\n",
    "# rng = default_rng(seed=1234)\n",
    "# data_size = M.shape[0]\n",
    "# utw = rng.multivariate_normal(mu, sigma, size=data_size*N.shape[1])\n",
    "# u = utw[:, 0:1]\n",
    "# from scipy import stats\n",
    "# z = stats.norm.cdf(utw[0:data_size, 2])[:, np.newaxis] \n",
    "# Z=z\n",
    "# Z = np.random.choice([0, 1], size=(data_size,), p=[1./3, 2./3])\n",
    "# x = stats.norm.cdf(utw[:, 1] + utw[:, 2] / np.sqrt(2))[:, np.newaxis]\n",
    "# # x = z + rng.normal(0, 0.01, data_size)[:, np.newaxis]\n",
    "# X_hidden=x.reshape(-1, N.shape[1])\n",
    "# structural = f(x)\n",
    "# Y_struct=structural\n",
    "# outcome = f(x) + u\n",
    "# Y=outcome\n",
    "\n",
    "# # Let's use gaussian error\n",
    "# data_size = X_hidden.shape[0]\n",
    "# std_X = np.std(X_hidden)\n",
    "# # Select scale_m and scale_n\n",
    "# # scale_m = 0.25\n",
    "# # scale_n = 1\n",
    "# # std_M, std_N = std_X * scale_m, std_X * scale_n\n",
    "# # M = X_hidden + std_M * np.random.normal(0, 1, data_size)[:, np.newaxis]\n",
    "# # N = X_hidden + std_N * np.random.normal(0, 1, data_size)[:, np.newaxis]\n",
    "\n",
    "# covariate = None\n",
    "# X_obs = None\n",
    "\n",
    "\n",
    "# Test data set\n",
    "# x_test = np.linspace(0, 1, 1000*N.shape[1])\n",
    "# y_test = f(x_test)\n",
    "# X_all_test = x_test.reshape(-1, N.shape[1])\n",
    "# Y_struct_test = y_test.reshape(-1, N.shape[1]).sum(axis=1)\n",
    "# covariate_test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d017192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc089e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mekiv method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2662f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "train_params = {'split_ratio': 0.5, \n",
    "                'lambda_mn': [0, -10], \n",
    "                'lambda_n': [0, -10],\n",
    "                'xi': [0, -10],\n",
    "                'lambda_x': None,\n",
    "                'n_chi': 500,\n",
    "                'Chi_lim': [-0.5, 0.5],\n",
    "                'label_cutoff': 1.0,\n",
    "                'reg_param': 0.,\n",
    "                'batch_size': 64, \n",
    "                'lr': 0.1, \n",
    "                'num_epochs': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51eec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***BEGIN STAGE 1***\n",
    "# Obtain training and testing indices\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainset1_idx, trainset2_idx = train_test_split(np.arange(X_test.shape[0]),\n",
    "                                                test_size = train_params['split_ratio'],\n",
    "                                                random_state = 1234)\n",
    "Z_trainset1 = pd.DataFrame(X_test.iloc[trainset1_idx,:]['z'])\n",
    "Z_trainset2 = pd.DataFrame(X_test.iloc[trainset2_idx,:]['z'])\n",
    "# Z_trainset1 = Z[trainset1_idx, np.newaxis] ; Z_trainset2 = Z[trainset1_idx, np.newaxis]\n",
    "M_trainset1 = M.iloc[trainset1_idx,:] ; M_trainset2 = M.iloc[trainset2_idx,:]\n",
    "N_trainset1 = N.iloc[trainset1_idx,:] ; N_trainset2 = N.iloc[trainset2_idx,:]\n",
    "X_hidden_trainset1 = X_test.iloc[trainset1_idx,3:]; X_hidden_trainset2 = X_test.iloc[trainset2_idx,3:]\n",
    "Y_trainset1 = pd.DataFrame(pd.DataFrame(y_test).iloc[trainset1_idx,:])\n",
    "Y_trainset2 = pd.DataFrame(pd.DataFrame(y_test).iloc[trainset2_idx,:])\n",
    "\n",
    "# No covariates used \n",
    "covariate_test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b72227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46966d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc6977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c635e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: obtain lambda and gamma via stage1_tuning function (trainer.py file)\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# The stage1_tuning function is used to obtain gamma and lambda\n",
    "# gamma_mn, lambda_mn = self.stage1_tuning(KMN1MN1, KMN1MN2, KZ1Z1, KZ1Z2, lambda_mn)\n",
    "# Method is as follows\n",
    "# Preliminaries\n",
    "# Initialize lambda mn and lambda n\n",
    "lambda_n = np.exp(np.linspace(train_params['lambda_n'][0], train_params['lambda_n'][1], 50))\n",
    "lambda_mn = np.exp(np.linspace(train_params['lambda_mn'][0], train_params['lambda_mn'][1], 50))\n",
    "\n",
    "# Obtain MN (concatenate along second axis)\n",
    "MN_trainset1 = np.c_[M_trainset1, N_trainset1] ; MN_trainset2 = np.c_[M_trainset2, N_trainset2] \n",
    "\n",
    "sigmaN = np.median(cdist(N_trainset1, N_trainset1, \"sqeuclidean\"))\n",
    "sigmaMN = np.median(cdist(MN_trainset1, MN_trainset1, \"sqeuclidean\"))\n",
    "sigmaZ = np.median(cdist(Z_trainset1, Z_trainset1, \"sqeuclidean\"))\n",
    "\n",
    "KZ1Z1 = np.exp(-cdist(Z_trainset1, Z_trainset1, \"sqeuclidean\") / 2 / float(sigmaZ))\n",
    "# torch: KZ1Z1 = torch.exp(-torch.cdist(Z1, Z1, \"sqeuclidean\") / 2 / float(sigmaZ))\n",
    "KZ1Z2 = np.exp(-cdist(Z_trainset1, Z_trainset2, \"sqeuclidean\") / 2 / float(sigmaZ))\n",
    "KN1N1 = np.exp(-cdist(N_trainset1, N_trainset1, \"sqeuclidean\") / 2 / float(sigmaN))\n",
    "KN1N2 = np.exp(-cdist(N_trainset1, N_trainset2, \"sqeuclidean\") / 2 / float(sigmaN))\n",
    "KMN1MN1 = np.exp(-cdist(MN_trainset1, MN_trainset1, \"sqeuclidean\") / 2 / float(sigmaMN))\n",
    "KMN1MN2 = np.exp(-cdist(MN_trainset1, MN_trainset2, \"sqeuclidean\") / 2 / float(sigmaMN))\n",
    "\n",
    "# Calculation\n",
    "n = Z_trainset1.shape[0]\n",
    "# N\n",
    "gamma_list = [np.linalg.solve(KZ1Z1 + n * lam1 * np.eye(n), KZ1Z2) for lam1 in lambda_n]\n",
    "score = [np.trace(gamma.T.dot(KN1N1.dot(gamma)) - 2 * KN1N2.T.dot(gamma)) for gamma in gamma_list]\n",
    "lambda_n = lambda_n[np.argmin(score)]\n",
    "gamma_n = gamma_list[np.argmin(score)]\n",
    "# MN\n",
    "gamma_list = [np.linalg.solve(KZ1Z1 + n * lam1 * np.eye(n), KZ1Z2) for lam1 in lambda_mn]\n",
    "score = [np.trace(gamma.T.dot(KMN1MN1.dot(gamma)) - 2 * KMN1MN2.T.dot(gamma)) for gamma in gamma_list]\n",
    "lambda_mn = lambda_mn[np.argmin(score)]\n",
    "gamma_mn = gamma_list[np.argmin(score)]\n",
    "\n",
    "# ***END STAGE 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e385d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90738146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9265e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***BEGIN Merror STAGE***\n",
    "# This is where the training happens with all the epochs and everything\n",
    "# **1**\n",
    "# The code uses the below function which we instead replace with its entire functionality\n",
    "# stageM_data = create_stage_M_raw_data(self.n_chi, N1, M1, Z2, gamma_n, gamma_mn, sigmaN, KZ1Z2)\n",
    "\n",
    "Chi_n = np.random.normal(0, 1, train_params['n_chi']* (N_trainset1.shape[1]))\n",
    "Chi_n = Chi_n / 2 / np.pi / sigmaN ** 0.5 # because the computed sigmaN is actually sigma^2N\n",
    "Chi_n = Chi_n.reshape(train_params['n_chi'],N_trainset1.shape[1])\n",
    "n, m = KZ1Z2.shape\n",
    "# Columns of Chi are repeated to account for the data size of the variable\n",
    "cos_term = np.cos(Chi_n @ N_trainset1.T)  # shape: Chi.shape[0] x args.train.N.shape[0]\n",
    "sin_term = np.sin(Chi_n @ N_trainset1.T)\n",
    "# Real (cos) and imaginary (sin) parts; dot products with gamma N\n",
    "denom = cos_term.dot(gamma_n) + sin_term.dot(gamma_n) * 1j \n",
    "# Component shape: Chi.shape[0] x args.dev.Z.shape[0]\n",
    "m_gamma_numer = sum([gamma_mn * M_trainset1.iloc[:,i].to_numpy().reshape(-1,1) for i in range(M_trainset1.shape[1])])\n",
    "numer = cos_term.dot(m_gamma_numer) + sin_term.dot(m_gamma_numer) * 1j \n",
    "raw_labels = (numer.to_numpy()/denom.to_numpy()).flatten().reshape(-1,1)\n",
    "raw_Chi = np.repeat(Chi_n, m).reshape(-1, N_trainset1.shape[1])\n",
    "raw_Z = np.repeat(Z_trainset2.to_numpy()[np.newaxis, :, :], train_params['n_chi'], axis=0).reshape(-1, Z_trainset2.shape[1])\n",
    "raw_dict = {'labels':raw_labels, 'Chi':raw_Chi, 'Z':raw_Z}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10779a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3fcc2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = np.real(raw_dict['labels']).flatten()\n",
    "imag_label = np.imag(raw_dict['labels']).flatten()\n",
    "idx_select = (real_label < np.mean(real_label) + 1. * np.std(real_label)) * (\n",
    "            real_label > np.mean(real_label) - 1. * np.std(real_label)) \\\n",
    "                 * (imag_label < np.mean(imag_label) + 1. * np.std(imag_label)) * (\n",
    "                         imag_label > np.mean(imag_label) - 1. * np.std(imag_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "779911fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "78306632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **2**\n",
    "# The code uses the below function which we instead replace with its entire functionality\n",
    "# stageM_data = prepare_stage_M_data(raw_data2=stageM_data, rand_seed=rand_seed)\n",
    "\n",
    "real_label = np.real(raw_dict['labels']).flatten()\n",
    "imag_label = np.imag(raw_dict['labels']).flatten()\n",
    "idx_select = (real_label < np.mean(real_label) + 1. * np.std(real_label)) * (\n",
    "            real_label > np.mean(real_label) - 1. * np.std(real_label)) \\\n",
    "                 * (imag_label < np.mean(imag_label) + 1. * np.std(imag_label)) * (\n",
    "                         imag_label > np.mean(imag_label) - 1. * np.std(imag_label))\n",
    "raw_labels = raw_dict['labels'][idx_select]\n",
    "raw_Chi = raw_dict['Chi'][idx_select]\n",
    "raw_Z = raw_dict['Z'][idx_select]\n",
    "shuffle_idx = np.arange(raw_Z.shape[0])\n",
    "np.random.default_rng(seed=1234).shuffle(shuffle_idx)\n",
    "for key in raw_dict.keys():\n",
    "    raw_dict[key][shuffle_idx]\n",
    "# The below code line just converts the data to torch if needed then adds new values to the class\n",
    "# Values added to class and converted to tensors\n",
    "# Pretty sure this isn't needed though because all components are manually converted to tensors in the code \n",
    "# StageMDataSetTorch.from_numpy(raw_data2)\n",
    "stageM_data = {'labels':raw_labels, 'Chi':raw_Chi, 'Z':raw_Z}\n",
    "stage1_MNZ = {'M': M_trainset1.to_numpy(), 'N': N_trainset1.to_numpy(), 'Z': Z_trainset1, 'sigmaZ': sigmaZ}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44535960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f07493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77be540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox. Code continues below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "018e99bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1750, 199])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(stage1_MNZ['N']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09c29a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1750, 9])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(stage1_MNZ['M']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9017769",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_initialiser = (torch.tensor(stage1_MNZ['M']) + torch.tensor(stage1_MNZ['N'])) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9e6ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_KZ1Z1 = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], stage1_MNZ['Z'], \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "test_K_Z1z = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], stageM_data['Z'][1:300], \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "test_gamma_x_I_lambda = sum([torch.eye(stage1_MNZ['Z'].shape[0]) * torch.exp(test_x_initialiser[:,i].reshape(-1,1)) for i in range(test_x_initialiser.shape[1])])\n",
    "test_gamma_x = torch.linalg.solve(test_KZ1Z1 + stage1_MNZ['Z'].shape[0] * test_gamma_x_I_lambda, test_K_Z1z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1d65bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002535357167000002"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(N.iloc[:,150:200].sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c355c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cos_term = torch.cos(torch.matmul(torch.tensor(stageM_data['Chi'][0:300].reshape(-1,1)).float(),test_x_initialiser.reshape(1, -1)))\n",
    "# test_sin_term = torch.sin(torch.matmul(torch.tensor(stageM_data['Chi'][0:300].reshape(-1,1)).float(),test_x_initialiser.reshape(1, -1)))\n",
    "\n",
    "test_cos_term = [torch.cos(torch.matmul(torch.tensor(stageM_data['Chi'][:,i].reshape(-1,1)[1:300]),\\\n",
    "                                          test_x_initialiser[:,i].reshape(1, -1))) for i in range(test_x_initialiser.shape[1])]\n",
    "test_sin_term = [torch.sin(torch.matmul(torch.tensor(stageM_data['Chi'][:,i].reshape(-1,1)[1:300]),\\\n",
    "                                          test_x_initialiser[:,i].reshape(1, -1))) for i in range(test_x_initialiser.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e7a229d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1750, 199])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_x_initialiser.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2950f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cos_w = torch.sum(test_cos_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "# test_sin_w = torch.sum(test_sin_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "\n",
    "test_cos_w = sum([torch.sum(test_cos_term[i] * test_gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(test_x_initialiser.shape[1])])\n",
    "test_sin_w = sum([torch.sum(test_sin_term[i] * test_gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(test_x_initialiser.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f3ffe7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_w_numer = sum([torch.sum(test_cos_term[i] * test_gamma_x.t() * test_x_initialiser[:,i].reshape(1, -1),\\\n",
    "                                               dim=-1).reshape(-1, 1) for i in range(test_x_initialiser.shape[1])])\n",
    "test_sin_w_numer = sum([torch.sum(test_sin_term[i] * test_gamma_x.t() * test_x_initialiser[:,i].reshape(1, -1),\\\n",
    "                                          dim=-1).reshape(-1, 1) for i in range(test_x_initialiser.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f5872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f040f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88761a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6d6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c13ab19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, batch     1] loss: 9837.06980, mse: 9837.06980, reg: 0.00000\n",
      "[epoch 1, batch     2] loss: 9817.21008, mse: 9817.21008, reg: 0.00000\n",
      "[epoch 1, batch     3] loss: 9797.48596, mse: 9797.48596, reg: 0.00000\n",
      "[epoch 1, batch     4] loss: 9777.52762, mse: 9777.52762, reg: 0.00000\n",
      "[epoch 1, batch     5] loss: 9757.95106, mse: 9757.95106, reg: 0.00000\n",
      "[epoch 1, batch     6] loss: 9738.28891, mse: 9738.28891, reg: 0.00000\n",
      "[epoch 1, batch     7] loss: 9718.60496, mse: 9718.60496, reg: 0.00000\n",
      "[epoch 1, batch     8] loss: 9698.93126, mse: 9698.93126, reg: 0.00000\n",
      "[epoch 1, batch     9] loss: 9679.28965, mse: 9679.28965, reg: 0.00000\n",
      "[epoch 1, batch    10] loss: 9659.85747, mse: 9659.85747, reg: 0.00000\n",
      "[epoch 1, batch    11] loss: 9640.31398, mse: 9640.31398, reg: 0.00000\n",
      "[epoch 1, batch    12] loss: 9620.84537, mse: 9620.84537, reg: 0.00000\n",
      "[epoch 1, batch    13] loss: 9601.17068, mse: 9601.17068, reg: 0.00000\n",
      "[epoch 1, batch    14] loss: 9581.61598, mse: 9581.61598, reg: 0.00000\n",
      "[epoch 1, batch    15] loss: 9562.13271, mse: 9562.13271, reg: 0.00000\n",
      "[epoch 1, batch    16] loss: 9542.73842, mse: 9542.73842, reg: 0.00000\n",
      "[epoch 1, batch    17] loss: 9523.50004, mse: 9523.50004, reg: 0.00000\n",
      "[epoch 1, batch    18] loss: 9503.95592, mse: 9503.95592, reg: 0.00000\n",
      "[epoch 1, batch    19] loss: 9484.52557, mse: 9484.52557, reg: 0.00000\n",
      "[epoch 1, batch    20] loss: 9465.21487, mse: 9465.21487, reg: 0.00000\n",
      "[epoch 1, batch    21] loss: 9446.23167, mse: 9446.23167, reg: 0.00000\n",
      "[epoch 1, batch    22] loss: 9426.54239, mse: 9426.54239, reg: 0.00000\n",
      "[epoch 1, batch    23] loss: 9407.66315, mse: 9407.66315, reg: 0.00000\n",
      "[epoch 1, batch    24] loss: 9388.03990, mse: 9388.03990, reg: 0.00000\n",
      "[epoch 1, batch    25] loss: 9368.86947, mse: 9368.86947, reg: 0.00000\n",
      "[epoch 1, batch    26] loss: 9349.83203, mse: 9349.83203, reg: 0.00000\n",
      "[epoch 1, batch    27] loss: 9330.51379, mse: 9330.51379, reg: 0.00000\n",
      "[epoch 1, batch    28] loss: 9311.14653, mse: 9311.14653, reg: 0.00000\n",
      "[epoch 1, batch    29] loss: 9292.25334, mse: 9292.25334, reg: 0.00000\n",
      "[epoch 1, batch    30] loss: 9273.16264, mse: 9273.16264, reg: 0.00000\n",
      "[epoch 1, batch    31] loss: 9253.98313, mse: 9253.98313, reg: 0.00000\n",
      "[epoch 1, batch    32] loss: 9234.76462, mse: 9234.76462, reg: 0.00000\n",
      "[epoch 1, batch    33] loss: 9215.63841, mse: 9215.63841, reg: 0.00000\n",
      "[epoch 1, batch    34] loss: 9195.98986, mse: 9195.98986, reg: 0.00000\n",
      "[epoch 1, batch    35] loss: 9177.04752, mse: 9177.04752, reg: 0.00000\n",
      "[epoch 1, batch    36] loss: 9157.78143, mse: 9157.78143, reg: 0.00000\n",
      "[epoch 1, batch    37] loss: 9138.00662, mse: 9138.00662, reg: 0.00000\n",
      "[epoch 1, batch    38] loss: 9119.93624, mse: 9119.93624, reg: 0.00000\n",
      "[epoch 1, batch    39] loss: 9099.83839, mse: 9099.83839, reg: 0.00000\n",
      "[epoch 1, batch    40] loss: 9080.55136, mse: 9080.55136, reg: 0.00000\n",
      "[epoch 1, batch    41] loss: 9061.83675, mse: 9061.83675, reg: 0.00000\n",
      "[epoch 1, batch    42] loss: 9041.58893, mse: 9041.58893, reg: 0.00000\n",
      "[epoch 1, batch    43] loss: 9023.59874, mse: 9023.59874, reg: 0.00000\n",
      "[epoch 1, batch    44] loss: 9003.65031, mse: 9003.65031, reg: 0.00000\n",
      "[epoch 1, batch    45] loss: 8984.60443, mse: 8984.60443, reg: 0.00000\n",
      "[epoch 1, batch    46] loss: 8964.94128, mse: 8964.94128, reg: 0.00000\n",
      "[epoch 1, batch    47] loss: 8945.76345, mse: 8945.76345, reg: 0.00000\n",
      "[epoch 1, batch    48] loss: 8927.05479, mse: 8927.05479, reg: 0.00000\n",
      "[epoch 1, batch    49] loss: 8909.00055, mse: 8909.00055, reg: 0.00000\n",
      "[epoch 1, batch    50] loss: 8890.21089, mse: 8890.21089, reg: 0.00000\n",
      "[epoch 1, batch    51] loss: 8870.20969, mse: 8870.20969, reg: 0.00000\n",
      "[epoch 1, batch    52] loss: 8852.43576, mse: 8852.43576, reg: 0.00000\n",
      "[epoch 1, batch    53] loss: 8833.54758, mse: 8833.54758, reg: 0.00000\n",
      "[epoch 1, batch    54] loss: 8814.26377, mse: 8814.26377, reg: 0.00000\n",
      "[epoch 1, batch    55] loss: 8797.06714, mse: 8797.06714, reg: 0.00000\n",
      "[epoch 1, batch    56] loss: 8776.02256, mse: 8776.02256, reg: 0.00000\n",
      "[epoch 1, batch    57] loss: 8756.51695, mse: 8756.51695, reg: 0.00000\n",
      "[epoch 1, batch    58] loss: 8737.48479, mse: 8737.48479, reg: 0.00000\n",
      "[epoch 1, batch    59] loss: 8719.80338, mse: 8719.80338, reg: 0.00000\n",
      "[epoch 1, batch    60] loss: 8700.22487, mse: 8700.22487, reg: 0.00000\n",
      "[epoch 1, batch    61] loss: 8682.13203, mse: 8682.13203, reg: 0.00000\n",
      "[epoch 1, batch    62] loss: 8662.88725, mse: 8662.88725, reg: 0.00000\n",
      "[epoch 1, batch    63] loss: 8640.22003, mse: 8640.22003, reg: 0.00000\n",
      "[epoch 1, batch    64] loss: 8621.78396, mse: 8621.78396, reg: 0.00000\n",
      "[epoch 1, batch    65] loss: 8604.78309, mse: 8604.78309, reg: 0.00000\n",
      "[epoch 1, batch    66] loss: 8586.95898, mse: 8586.95898, reg: 0.00000\n",
      "[epoch 1, batch    67] loss: 8566.17176, mse: 8566.17176, reg: 0.00000\n",
      "[epoch 1, batch    68] loss: 8547.99787, mse: 8547.99787, reg: 0.00000\n",
      "[epoch 1, batch    69] loss: 8527.91264, mse: 8527.91264, reg: 0.00000\n",
      "[epoch 1, batch    70] loss: 8507.15073, mse: 8507.15073, reg: 0.00000\n",
      "[epoch 1, batch    71] loss: 8489.37949, mse: 8489.37949, reg: 0.00000\n",
      "[epoch 1, batch    72] loss: 8464.94601, mse: 8464.94601, reg: 0.00000\n",
      "[epoch 1, batch    73] loss: 8447.13161, mse: 8447.13161, reg: 0.00000\n",
      "[epoch 1, batch    74] loss: 8427.26921, mse: 8427.26921, reg: 0.00000\n",
      "[epoch 1, batch    75] loss: 8405.70741, mse: 8405.70741, reg: 0.00000\n",
      "[epoch 1, batch    76] loss: 8387.93135, mse: 8387.93135, reg: 0.00000\n",
      "[epoch 1, batch    77] loss: 8367.98420, mse: 8367.98420, reg: 0.00000\n",
      "[epoch 1, batch    78] loss: 8344.79798, mse: 8344.79798, reg: 0.00000\n",
      "[epoch 1, batch    79] loss: 8323.43147, mse: 8323.43147, reg: 0.00000\n",
      "[epoch 1, batch    80] loss: 8305.53671, mse: 8305.53671, reg: 0.00000\n",
      "[epoch 1, batch    81] loss: 8281.71521, mse: 8281.71521, reg: 0.00000\n",
      "[epoch 1, batch    82] loss: 8261.80731, mse: 8261.80731, reg: 0.00000\n",
      "[epoch 1, batch    83] loss: 8239.73691, mse: 8239.73691, reg: 0.00000\n",
      "[epoch 1, batch    84] loss: 8218.02770, mse: 8218.02770, reg: 0.00000\n",
      "[epoch 1, batch    85] loss: 8195.68626, mse: 8195.68626, reg: 0.00000\n",
      "[epoch 1, batch    86] loss: 8174.98383, mse: 8174.98383, reg: 0.00000\n",
      "[epoch 1, batch    87] loss: 8151.36207, mse: 8151.36207, reg: 0.00000\n",
      "[epoch 1, batch    88] loss: 8132.67717, mse: 8132.67717, reg: 0.00000\n",
      "[epoch 1, batch    89] loss: 8108.33751, mse: 8108.33751, reg: 0.00000\n",
      "[epoch 1, batch    90] loss: 8086.12364, mse: 8086.12364, reg: 0.00000\n",
      "[epoch 1, batch    91] loss: 8064.70784, mse: 8064.70784, reg: 0.00000\n",
      "[epoch 1, batch    92] loss: 8043.47759, mse: 8043.47759, reg: 0.00000\n",
      "[epoch 1, batch    93] loss: 8015.28150, mse: 8015.28150, reg: 0.00000\n",
      "[epoch 1, batch    94] loss: 7993.53248, mse: 7993.53248, reg: 0.00000\n",
      "[epoch 1, batch    95] loss: 7969.81411, mse: 7969.81411, reg: 0.00000\n",
      "[epoch 1, batch    96] loss: 7943.25184, mse: 7943.25184, reg: 0.00000\n",
      "[epoch 1, batch    97] loss: 7922.06555, mse: 7922.06555, reg: 0.00000\n",
      "[epoch 1, batch    98] loss: 7897.24807, mse: 7897.24807, reg: 0.00000\n",
      "[epoch 1, batch    99] loss: 7872.37829, mse: 7872.37829, reg: 0.00000\n",
      "[epoch 1, batch   100] loss: 7847.38613, mse: 7847.38613, reg: 0.00000\n",
      "[epoch 1, batch   101] loss: 7819.59894, mse: 7819.59894, reg: 0.00000\n",
      "[epoch 1, batch   102] loss: 7796.16406, mse: 7796.16406, reg: 0.00000\n",
      "[epoch 1, batch   103] loss: 7771.85504, mse: 7771.85504, reg: 0.00000\n",
      "[epoch 1, batch   104] loss: 7744.17126, mse: 7744.17126, reg: 0.00000\n",
      "[epoch 1, batch   105] loss: 7722.05335, mse: 7722.05335, reg: 0.00000\n",
      "[epoch 1, batch   106] loss: 7695.89213, mse: 7695.89213, reg: 0.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse \u001b[38;5;241m+\u001b[39m train_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_param\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m reg\n\u001b[1;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 111\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    113\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# **3** \n",
    "\n",
    "# stage_m_out = self.stage_M_main(stageM_data=stageM_data, stage1_MNZ=stage1_MNZ, train_params=self.train_params)\n",
    "class model_class(torch.nn.Module):\n",
    "    def __init__(self, stageM_data: stageM_data, train_params: train_params, stage1_MNZ: stage1_MNZ,\n",
    "                 gpu_flg: bool = False):\n",
    "        super().__init__()\n",
    "        self.stageM_data = stageM_data\n",
    "        self.stage1_MNZ = stage1_MNZ\n",
    "        self.reg_param = train_params['reg_param']\n",
    "        # We are attempting to uncover a 1 dimensional X and thus initialize with the row averages of M and N \n",
    "#       self.x_initialiser = (torch.tensor(stage1_MNZ['M']).mean(axis=1) + torch.tensor(stage1_MNZ['N']).mean(axis=1)) / 2\n",
    "        # Multidimensional X with the same dimensions as N    \n",
    "        self.x_initialiser = (torch.tensor(stage1_MNZ['M']) + torch.tensor(stage1_MNZ['N'])) / 2\n",
    "\n",
    "        if not train_params['lambda_x']:\n",
    "#             self.params = torch.nn.Parameter(self.x_initialiser.flatten())\n",
    "#             self.x = self.params.reshape(-1,1)\n",
    "            self.x = torch.nn.Parameter(self.x_initialiser)\n",
    "            self.lambda_x = self.x\n",
    "        else:\n",
    "#             self.params = torch.nn.Parameter(self.x_initialiser.flatten())\n",
    "#             self.x = self.params.reshape(-1,1)\n",
    "            self.x = torch.nn.Parameter(self.x_initialiser)\n",
    "            self.lambda_x = train_params['lambda_x']\n",
    "        self.train_params = train_params\n",
    "        self.KZ1Z1 = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], stage1_MNZ['Z'], \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "    def forward(self, idx):\n",
    "        ### gamma ###\n",
    "        n = self.stage1_MNZ['Z'].shape[0]\n",
    "        z = self.stageM_data['Z'][idx]\n",
    "        K_Z1z = torch.tensor(np.exp(-cdist(stage1_MNZ['Z'], z, \"sqeuclidean\") / 2 / float(stage1_MNZ['sigmaZ'])))\n",
    "        # gamma = self.cme_X.brac_inv.matmul(K_Zz)\n",
    "        if not self.train_params[\"lambda_x\"]:\n",
    "            gamma_x_I_lambda = sum([torch.eye(n) * torch.exp(self.lambda_x[:,i].reshape(-1,1)) for i in range(self.lambda_x.shape[1])])\n",
    "            gamma_x = torch.linalg.solve(self.KZ1Z1 + n * gamma_x_I_lambda, K_Z1z)\n",
    "#             gamma_x = torch.linalg.solve(self.KZ1Z1 + n * torch.exp(self.lambda_x) * torch.eye(n), K_Z1z)\n",
    "            # gamma_x = torch.linalg.solve(self.KZ1Z1 + n * self.lambda_x * torch.eye(n), K_Z1z)\n",
    "        else:\n",
    "            gamma_x_I_lambda = sum([torch.eye(n) * torch.exp(self.lambda_x[:,i].reshape(-1,1)) for i in range(self.lambda_x.shape[1])])\n",
    "            gamma_x = torch.linalg.solve(self.KZ1Z1 + n * gamma_x_I_lambda, K_Z1z)\n",
    "#             gamma_x = torch.linalg.solve(self.KZ1Z1 + n * self.lambda_x * torch.eye(n), K_Z1z)\n",
    "\n",
    "\n",
    "        ### decompose e^{i\\mathcal{X}n_i} ###\n",
    "        cos_term = [torch.cos(torch.matmul(torch.tensor(self.stageM_data['Chi'][:,i].reshape(-1,1)[idx]),\\\n",
    "                                          self.x[:,i].reshape(1, -1))) for i in range(self.x.shape[1])]\n",
    "        sin_term = [torch.sin(torch.matmul(torch.tensor(self.stageM_data['Chi'][:,i].reshape(-1,1)[idx]),\\\n",
    "                                          self.x[:,i].reshape(1, -1))) for i in range(self.x.shape[1])]\n",
    "#         cos_term = torch.cos(torch.matmul(torch.tensor(self.stageM_data['Chi'].reshape(-1,1)[idx]).float(),\\\n",
    "#                                           self.x.reshape(1, -1)))\n",
    "#         sin_term = torch.sin(torch.matmul(torch.tensor(self.stageM_data['Chi'].reshape(-1,1)[idx]).float(),\\\n",
    "#                                           self.x.reshape(1, -1)))\n",
    "\n",
    "        denom = {}\n",
    "        # using gamma to evaluate the charasteristic function value at a bunch of curly_x's\n",
    "        denom['cos_weighted'] = sum([torch.sum(cos_term[i] * gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(self.x.shape[1])])\n",
    "        denom['sin_weighted'] = sum([torch.sum(sin_term[i] * gamma_x.t(), dim=-1).reshape(-1, 1)\\\n",
    "                                     for i in range(self.x.shape[1])])\n",
    "#         denom['cos_weighted'] = torch.sum(cos_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "#         denom['sin_weighted'] = torch.sum(sin_term * gamma_x.t(), dim=-1).reshape(-1, 1)\n",
    "        denom['value'] = denom['cos_weighted'] + denom['sin_weighted'] * 1j\n",
    "\n",
    "        numer = {}\n",
    "        numer['cos_weighted'] = sum([torch.sum(cos_term[i] * gamma_x.t() * self.x[:,i].reshape(1, -1),\\\n",
    "                                               dim=-1).reshape(-1, 1) for i in range(self.x.shape[1])])\n",
    "        numer['sin_weighted'] = sum([torch.sum(sin_term[i] * gamma_x.t() * self.x[:,i].reshape(1, -1),\\\n",
    "                                          dim=-1).reshape(-1, 1) for i in range(self.x.shape[1])])\n",
    "#         numer['cos_weighted'] = torch.sum(cos_term * gamma_x.t() * self.x.reshape(1, -1), dim=-1).reshape(-1, 1)\n",
    "#         numer['sin_weighted'] = torch.sum(sin_term * gamma_x.t() * self.x.reshape(1, -1), dim=-1).reshape(-1, 1)\n",
    "        numer['value'] = numer['cos_weighted'] + numer['sin_weighted'] * 1j\n",
    "\n",
    "        return numer['value'] / denom['value']\n",
    "\n",
    "model = model_class(stageM_data=stageM_data, train_params=train_params, stage1_MNZ=stage1_MNZ)\n",
    "\n",
    "model.train() # tells your model that you are training the model, not evaluating it\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_params['lr'])\n",
    "\n",
    "losses = []\n",
    "early_stop = False\n",
    "step = 0\n",
    "for ep in range(train_params['num_epochs']):\n",
    "    if early_stop:\n",
    "        break\n",
    "    running_loss = 0.0\n",
    "    batches_idxes = []\n",
    "    idxes = np.arange((stageM_data['Chi']).shape[0])\n",
    "    np.random.shuffle(idxes)\n",
    "    batch_i = 0\n",
    "    while True:\n",
    "        batches_idxes.append(torch.tensor(idxes[batch_i * train_params['batch_size']: (batch_i + 1) * train_params['batch_size']]))\n",
    "        batch_i += 1\n",
    "        if batch_i * train_params['batch_size'] >= (stageM_data['Chi']).shape[0]:\n",
    "            break\n",
    "    for i, batch_idx in enumerate(batches_idxes):\n",
    "        preds = model(batch_idx)\n",
    "        # Loss functionality\n",
    "        labels = stageM_data['labels'][batch_idx]\n",
    "        dim_label = labels.shape[-1]\n",
    "        num_label = labels.shape[0]\n",
    "        preds_as_real = torch.view_as_real(preds)\n",
    "        labels_as_real = torch.view_as_real(torch.tensor(labels))\n",
    "        mse = torch.sum((labels_as_real - preds_as_real) ** 2) / num_label / dim_label\n",
    "#       reg = torch.sum((model.x - (torch.tensor(stage1_MNZ['M'].mean(axis=1) + stage1_MNZ['N'].mean(axis=1)) / 2)) ** 2)\n",
    "        reg = torch.sum((model.x - (torch.tensor(stage1_MNZ['M'] + stage1_MNZ['N']) / 2))** 2)\n",
    "        loss = mse + train_params['reg_param'] * reg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print('[epoch %d, batch %5d] loss: %.5f, mse: %.5f, reg: %.5f' % (\n",
    "            ep + 1, i + 1, running_loss / 1, mse / 1, train_params['reg_param'] * reg / 1))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if step > 8000: # Max of 8000 iterations\n",
    "            break\n",
    "        if (step > 2) and np.abs(losses[-1] - losses[-2]) < 1e-7: # Convergence considered to be < 1e-7\n",
    "            early_stop = True\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "\n",
    "# Convert model to numpy after training\n",
    "fitted_x = model.x.detach().numpy()\n",
    "#  assert stage_M_out.fitted_x.shape[0] == stage1_MNZ.Z.shape[0]\n",
    "if not train_params['lambda_x']:\n",
    "    lambda_x = np.exp(model.lambda_x.detach().numpy())  # syntax?\n",
    "else:\n",
    "    lambda_x = model.lambda_x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **4**\n",
    "gamma_x = sum([np.linalg.solve(KZ1Z1 + n * lambda_x[:,i] * np.eye(n), KZ1Z2) for i in range(lambda_x.shape[1])])\n",
    "#gamma_x = np.linalg.solve(KZ1Z1 + n * lambda_x * np.eye(n), KZ1Z2)\n",
    "sigmaX = np.median(cdist(fitted_x, fitted_x, \"sqeuclidean\"))\n",
    "KfittedX = np.exp(-cdist(fitted_x, fitted_x, \"sqeuclidean\") / 2 / float(sigmaX))\n",
    "W = KfittedX.dot(gamma_x)\n",
    "\n",
    "# ***END Merror STAGE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71541bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***BEGIN STAGE 2***\n",
    "\n",
    "xi = train_params['xi']\n",
    "if isinstance(xi, list):\n",
    "    xi = np.exp(np.linspace(xi[0], xi[1], 50))\n",
    "    S = W.shape[1]\n",
    "    b = W.dot(Y_trainset2)\n",
    "    A = W.dot(W.T) # W.T is transpose of W\n",
    "    alpha_list = [np.linalg.solve(A + S * lam2 * KfittedX, b) for lam2 in xi]\n",
    "    score = [np.linalg.norm(Y_trainset1 - KfittedX.dot(alpha)) for alpha in alpha_list]\n",
    "    alpha = alpha_list[np.argmin(score)]\n",
    "    xi = xi[np.argmin(score)]\n",
    "else:\n",
    "    alpha = np.linalg.solve(W.dot(W.T) + m * self.xi * KfittedX, W.dot(Y_trainset2))\n",
    "    \n",
    "# ***END STAGE 2***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a22ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***OBTAIN FINAL OUTPUT***\n",
    "# Concatenate the covariate with the test data if there is a covariate\n",
    "X_hidden_trainset2 = X_hidden_trainset2.iloc[:,:-1]\n",
    "if covariate_test is not None:\n",
    "    X_hidden_trainset2 = np.concatenate([X_hidden_trainset2, covariate_test], axis=-1)\n",
    "# Obtain predictions \n",
    "Kx = np.exp(-cdist(X_hidden_trainset2, fitted_x, \"sqeuclidean\") / 2 / float(sigmaX))\n",
    "preds = np.dot(Kx, alpha)\n",
    "# Evaluate the model \n",
    "mse = np.mean((Y_trainset2 - preds)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d345e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a36a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29aa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba63e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f58894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc6331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c71babd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201.5096159526795\n"
     ]
    }
   ],
   "source": [
    "# Old mse (random setup with no relation between variables, just trying to make multidimensional work)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4baaf095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4876152472680255\n"
     ]
    }
   ],
   "source": [
    "# Old mse (1 dimensional X)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982d2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
